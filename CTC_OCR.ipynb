{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "mLLOYakSwD00"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import re\n",
        "import argparse\n",
        "import pprint\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "import yaml\n",
        "from pathlib import Path\n",
        "import cv2\n",
        "import gdown\n",
        "import zipfile\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download dataset from google drive\n",
        "\n",
        "\n",
        "> ocr_dataset.zip https://drive.google.com/file/d/1SOk3XX7C9AC_HmVzijV6CPdVbFhQUKMK/view?usp=drive_link\n",
        "\n",
        "> labels.txt https://drive.google.com/file/d/1o9FDcQJG19C4m5RfqggDOX8ZavsRb3jz/view?usp=drive_link\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "soVQspK-xD2a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# File IDs\n",
        "dataset_zip_id = '1SOk3XX7C9AC_HmVzijV6CPdVbFhQUKMK'\n",
        "labels_txt_id = '1o9FDcQJG19C4m5RfqggDOX8ZavsRb3jz'\n",
        "\n",
        "# Output filenames\n",
        "dataset_zip_filename = 'ocr_dataset.zip'\n",
        "labels_txt_filename = 'labels.txt'\n",
        "dataset_folder = 'ocr_dataset'\n",
        "# Remove the 'ocr_dataset' folder if it exists\n",
        "if os.path.exists(dataset_folder):\n",
        "    shutil.rmtree(dataset_folder)\n",
        "\n",
        "# Download the dataset ZIP file\n",
        "gdown.download(f'https://drive.google.com/uc?id={dataset_zip_id}', dataset_zip_filename, quiet=False)\n",
        "\n",
        "# Extract the dataset ZIP file\n",
        "with zipfile.ZipFile(dataset_zip_filename, 'r') as zip_ref:\n",
        "    zip_ref.extractall()\n",
        "\n",
        "# Download the labels.txt file\n",
        "gdown.download(f'https://drive.google.com/uc?id={labels_txt_id}', labels_txt_filename, quiet=False)\n",
        "\n",
        "# Optionally, you can remove the downloaded ZIP file\n",
        "os.remove(dataset_zip_filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCsrvzU0xCQ1",
        "outputId": "bd99cd03-6086-48e7-9aed-00a371338bde"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1SOk3XX7C9AC_HmVzijV6CPdVbFhQUKMK\n",
            "To: /content/ocr_dataset.zip\n",
            "100%|██████████| 133M/133M [00:00<00:00, 194MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1o9FDcQJG19C4m5RfqggDOX8ZavsRb3jz\n",
            "To: /content/labels.txt\n",
            "100%|██████████| 288/288 [00:00<00:00, 1.13MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to your dataset\n",
        "dataset_path = \"ocr_dataset\"\n",
        "\n",
        "# Create an empty list to store tuples of (filename, label)\n",
        "all_entries = []\n",
        "\n",
        "# Iterate through each folder in the dataset\n",
        "for folder_name in os.listdir(dataset_path):\n",
        "    folder_path = os.path.join(dataset_path, folder_name)\n",
        "    # Check if it's a directory\n",
        "    if os.path.isdir(folder_path):\n",
        "        # Read the labels from the 'labels.txt' file\n",
        "        labels_file_path = os.path.join(folder_path, \"labels.txt\")\n",
        "        with open(labels_file_path, \"r\") as labels_file:\n",
        "            lines = labels_file.readlines()\n",
        "\n",
        "            # Create a list of tuples containing image filenames and labels\n",
        "            for line in lines:\n",
        "                words = line.split()\n",
        "                if len(words) > 1:\n",
        "                    filename = f\"content/{dataset_path}/{folder_name}/{words[0]}\"\n",
        "                    label = \" \".join(words[1:])\n",
        "                    all_entries.append((filename, label.strip()))\n",
        "\n",
        "\n",
        "# Define the ratio of entries for training (80%) and validation (20%)\n",
        "split_ratio = 0.8\n",
        "split_index = int(len(all_entries) * split_ratio)\n",
        "\n",
        "# Separate entries into training and validation sets\n",
        "train_entries = all_entries[:split_index]\n",
        "val_entries = all_entries[split_index:]\n",
        "\n",
        "\n",
        "# Function to write entries to a text file\n",
        "def write_to_file(entries, file_path):\n",
        "    with open(file_path, \"w\") as file:\n",
        "        for filename, label in entries:\n",
        "            file.write(f\"{filename} {label}\\n\")\n",
        "\n",
        "\n",
        "# Define paths for annotation files\n",
        "train_annotation_path = \"train_annotation.txt\"\n",
        "val_annotation_path = \"val_annotation.txt\"\n",
        "\n",
        "# Write entries to annotation files\n",
        "write_to_file(train_entries, train_annotation_path)\n",
        "write_to_file(val_entries, val_annotation_path)\n",
        "\n",
        "print(f\"Training annotation file created: {train_annotation_path}\")\n",
        "print(f\"Validation annotation file created: {val_annotation_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHgv5WzM1ZsA",
        "outputId": "7f66c89b-371d-44da-8d1c-4c92fd577ee2"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training annotation file created: train_annotation.txt\n",
            "Validation annotation file created: val_annotation.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "try:\n",
        "    AUTOTUNE = tf.data.AUTOTUNE\n",
        "except AttributeError:\n",
        "    # tf < 2.4.0\n",
        "    AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "\n",
        "class Dataset(tf.data.TextLineDataset):\n",
        "    def __init__(self, filename, **kwargs):\n",
        "        self.dirname = os.path.dirname(filename)\n",
        "        super().__init__(filename, **kwargs)\n",
        "\n",
        "    def parse_func(self, line):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def parse_line(self, line):\n",
        "        line = tf.strings.strip(line)\n",
        "        img_relative_path, label = self.parse_func(line)\n",
        "        img_path = tf.strings.join([self.dirname, os.sep, img_relative_path])\n",
        "        return img_path, label\n",
        "\n",
        "\n",
        "class SimpleDataset(Dataset):\n",
        "    def parse_func(self, line):\n",
        "        splited_line = tf.strings.split(line)\n",
        "        img_relative_path, label = splited_line[0], splited_line[1]\n",
        "        return img_relative_path, label\n",
        "\n",
        "\n",
        "class MJSynthDataset(Dataset):\n",
        "    def parse_func(self, line):\n",
        "        splited_line = tf.strings.split(line)\n",
        "        img_relative_path = splited_line[0]\n",
        "        label = tf.strings.split(img_relative_path, sep=\"_\")[1]\n",
        "        return img_relative_path, label\n",
        "\n",
        "\n",
        "class ICDARDataset(Dataset):\n",
        "    def parse_func(self, line):\n",
        "        splited_line = tf.strings.split(line, sep=\",\")\n",
        "        img_relative_path, label = splited_line[0], splited_line[1]\n",
        "        label = tf.strings.strip(label)\n",
        "        label = tf.strings.regex_replace(label, r'\"', \"\")\n",
        "        return img_relative_path, label\n"
      ],
      "metadata": {
        "id": "ErSJW96kwFYF"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class DatasetBuilder:\n",
        "    def __init__(\n",
        "        self,\n",
        "        table_path,\n",
        "        img_shape=(32, None, 3),\n",
        "        max_img_width=300,\n",
        "        ignore_case=False,\n",
        "    ):\n",
        "        # map unknown label to 0\n",
        "        self.table = tf.lookup.StaticHashTable(\n",
        "            tf.lookup.TextFileInitializer(\n",
        "                table_path,\n",
        "                tf.string,\n",
        "                tf.lookup.TextFileIndex.WHOLE_LINE,\n",
        "                tf.int64,\n",
        "                tf.lookup.TextFileIndex.LINE_NUMBER,\n",
        "            ),\n",
        "            0,\n",
        "        )\n",
        "        self.img_shape = img_shape\n",
        "        self.ignore_case = ignore_case\n",
        "        if img_shape[1] is None:\n",
        "            self.max_img_width = max_img_width\n",
        "            self.preserve_aspect_ratio = True\n",
        "        else:\n",
        "            self.preserve_aspect_ratio = False\n",
        "\n",
        "    @property\n",
        "    def num_classes(self):\n",
        "        return self.table.size()\n",
        "\n",
        "    def _parse_annotation(self, path):\n",
        "        with open(path) as f:\n",
        "            line = f.readline().strip()\n",
        "        if re.fullmatch(r\".*/*\\d+_.+_(\\d+)\\.\\w+ \\1\", line):\n",
        "            return MJSynthDataset(path)\n",
        "        elif re.fullmatch(r'.*/*word_\\d\\.\\w+, \".+\"', line):\n",
        "            return ICDARDataset(path)\n",
        "        elif re.fullmatch(r\".+\\.\\w+ .+\", line):\n",
        "            return SimpleDataset(path)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported annotation format\")\n",
        "\n",
        "    def _concatenate_ds(self, ann_paths):\n",
        "        datasets = [self._parse_annotation(path) for path in ann_paths]\n",
        "        concatenated_ds = datasets[0].map(datasets[0].parse_line)\n",
        "        for ds in datasets[1:]:\n",
        "            ds = ds.map(ds.parse_line)\n",
        "            concatenated_ds = concatenated_ds.concatenate(ds)\n",
        "        return concatenated_ds\n",
        "\n",
        "    def _decode_img(self, filename, label):\n",
        "        img = tf.io.read_file(filename)\n",
        "        img = tf.io.decode_jpeg(img, channels=self.img_shape[-1])\n",
        "        if self.preserve_aspect_ratio:\n",
        "            img_shape = tf.shape(img)\n",
        "            scale_factor = self.img_shape[0] / img_shape[0]\n",
        "            img_width = scale_factor * tf.cast(img_shape[1], tf.float64)\n",
        "            img_width = tf.cast(img_width, tf.int32)\n",
        "        else:\n",
        "            img_width = self.img_shape[1]\n",
        "        img = tf.image.resize(img, (self.img_shape[0], img_width)) / 255.0\n",
        "        return img, label\n",
        "\n",
        "    def _filter_img(self, img, label):\n",
        "        img_shape = tf.shape(img)\n",
        "        return img_shape[1] < self.max_img_width\n",
        "\n",
        "    def _tokenize(self, imgs, labels):\n",
        "        chars = tf.strings.unicode_split(labels, \"UTF-8\")\n",
        "        tokens = tf.ragged.map_flat_values(self.table.lookup, chars)\n",
        "        # TODO(hym) Waiting for official support to use RaggedTensor in keras\n",
        "        tokens = tokens.to_sparse()\n",
        "        return imgs, tokens\n",
        "\n",
        "    def __call__(self, ann_paths, batch_size, is_training):\n",
        "        ds = self._concatenate_ds(ann_paths)\n",
        "        if self.ignore_case:\n",
        "            ds = ds.map(lambda x, y: (x, tf.strings.lower(y)))\n",
        "        if is_training:\n",
        "            ds = ds.shuffle(buffer_size=10000)\n",
        "        ds = ds.map(self._decode_img, AUTOTUNE)\n",
        "        if self.preserve_aspect_ratio and batch_size != 1:\n",
        "            ds = ds.filter(self._filter_img)\n",
        "            ds = ds.padded_batch(batch_size, drop_remainder=is_training)\n",
        "        else:\n",
        "            ds = ds.batch(batch_size, drop_remainder=is_training)\n",
        "        ds = ds.map(self._tokenize, AUTOTUNE)\n",
        "        ds = ds.prefetch(AUTOTUNE)\n",
        "        return ds\n"
      ],
      "metadata": {
        "id": "Uo5pfROlwFa3"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def vgg_style(x):\n",
        "    x = tf.keras.layers.Conv2D(64, 3, padding=\"same\", activation=\"relu\", name=\"conv1\")(\n",
        "        x\n",
        "    )\n",
        "    x = tf.keras.layers.MaxPool2D(pool_size=2, padding=\"same\", name=\"pool1\")(x)\n",
        "    x = tf.keras.layers.Conv2D(128, 3, padding=\"same\", activation=\"relu\", name=\"conv2\")(\n",
        "        x\n",
        "    )\n",
        "    x = tf.keras.layers.MaxPool2D(pool_size=2, padding=\"same\", name=\"pool2\")(x)\n",
        "    x = tf.keras.layers.Conv2D(256, 3, padding=\"same\", use_bias=False, name=\"conv3\")(x)\n",
        "    x = tf.keras.layers.BatchNormalization(name=\"bn3\")(x)\n",
        "    x = tf.keras.layers.Activation(\"relu\", name=\"relu3\")(x)\n",
        "    x = tf.keras.layers.Conv2D(256, 3, padding=\"same\", activation=\"relu\", name=\"conv4\")(\n",
        "        x\n",
        "    )\n",
        "    x = tf.keras.layers.MaxPool2D(\n",
        "        pool_size=2, strides=(2, 1), padding=\"same\", name=\"pool4\"\n",
        "    )(x)\n",
        "    x = tf.keras.layers.Conv2D(512, 3, padding=\"same\", use_bias=False, name=\"conv5\")(x)\n",
        "    x = tf.keras.layers.BatchNormalization(name=\"bn5\")(x)\n",
        "    x = tf.keras.layers.Activation(\"relu\", name=\"relu5\")(x)\n",
        "    x = tf.keras.layers.Conv2D(512, 3, padding=\"same\", activation=\"relu\", name=\"conv6\")(\n",
        "        x\n",
        "    )\n",
        "    x = tf.keras.layers.MaxPool2D(\n",
        "        pool_size=2, strides=(2, 1), padding=\"same\", name=\"pool6\"\n",
        "    )(x)\n",
        "    x = tf.keras.layers.Conv2D(512, 2, use_bias=False, name=\"conv7\")(x)\n",
        "    x = tf.keras.layers.BatchNormalization(name=\"bn7\")(x)\n",
        "    x = tf.keras.layers.Activation(\"relu\", name=\"relu7\")(x)\n",
        "    x = tf.keras.layers.Reshape((-1, 512), name=\"reshape7\")(x)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "DcXWyOVFwFd9"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def build_model(\n",
        "    num_classes,\n",
        "    weight=None,\n",
        "    preprocess=None,\n",
        "    postprocess=None,\n",
        "    img_shape=(64, 512, 3),  # Update the input shape\n",
        "    model_name=\"crnn\",\n",
        "):\n",
        "    x = img_input = tf.keras.Input(shape=img_shape)\n",
        "    if preprocess is not None:\n",
        "        x = preprocess(x)\n",
        "\n",
        "    x = vgg_style(x)\n",
        "    x = tf.keras.layers.Bidirectional(\n",
        "        tf.keras.layers.LSTM(units=256, return_sequences=True), name=\"bi_lstm1\"\n",
        "    )(x)\n",
        "    x = tf.keras.layers.Bidirectional(\n",
        "        tf.keras.layers.LSTM(units=256, return_sequences=True), name=\"bi_lstm2\"\n",
        "    )(x)\n",
        "    x = tf.keras.layers.Dense(units=num_classes, name=\"logits\")(x)\n",
        "\n",
        "    if postprocess is not None:\n",
        "        x = postprocess(x)\n",
        "\n",
        "    model = tf.keras.Model(inputs=img_input, outputs=x, name=model_name)\n",
        "    if weight is not None:\n",
        "        model.load_weights(weight, by_name=True, skip_mismatch=True)\n",
        "    return model"
      ],
      "metadata": {
        "id": "0TGoqTUTwFhM"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class SequenceAccuracy(tf.keras.metrics.Metric):\n",
        "    def __init__(self, name=\"sequence_accuracy\", **kwargs):\n",
        "        super().__init__(name=name, **kwargs)\n",
        "        self.total = self.add_weight(name=\"total\", initializer=\"zeros\")\n",
        "        self.count = self.add_weight(name=\"count\", initializer=\"zeros\")\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        def sparse2dense(tensor, shape):\n",
        "            tensor = tf.sparse.reset_shape(tensor, shape)\n",
        "            tensor = tf.sparse.to_dense(tensor, default_value=-1)\n",
        "            tensor = tf.cast(tensor, tf.float32)\n",
        "            return tensor\n",
        "\n",
        "        y_true_shape = tf.shape(y_true)\n",
        "        batch_size = y_true_shape[0]\n",
        "        y_pred_shape = tf.shape(y_pred)\n",
        "        max_width = tf.math.maximum(y_true_shape[1], y_pred_shape[1])\n",
        "        logit_length = tf.fill([batch_size], y_pred_shape[1])\n",
        "        decoded, _ = tf.nn.ctc_greedy_decoder(\n",
        "            inputs=tf.transpose(y_pred, perm=[1, 0, 2]),\n",
        "            sequence_length=logit_length,\n",
        "        )\n",
        "        y_true = sparse2dense(y_true, [batch_size, max_width])\n",
        "        y_pred = sparse2dense(decoded[0], [batch_size, max_width])\n",
        "        num_errors = tf.math.reduce_any(tf.math.not_equal(y_true, y_pred), axis=1)\n",
        "        num_errors = tf.cast(num_errors, tf.float32)\n",
        "        num_errors = tf.math.reduce_sum(num_errors)\n",
        "        batch_size = tf.cast(batch_size, tf.float32)\n",
        "        self.total.assign_add(batch_size)\n",
        "        self.count.assign_add(batch_size - num_errors)\n",
        "\n",
        "    def result(self):\n",
        "        return self.count / self.total\n",
        "\n",
        "    def reset_states(self):\n",
        "        self.count.assign(0)\n",
        "        self.total.assign(0)\n"
      ],
      "metadata": {
        "id": "ShmZvJRwwtw6"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class EditDistance(tf.keras.metrics.Metric):\n",
        "    def __init__(self, name=\"edit_distance\", **kwargs):\n",
        "        super().__init__(name=name, **kwargs)\n",
        "        self.total = self.add_weight(name=\"total\", initializer=\"zeros\")\n",
        "        self.sum_distance = self.add_weight(name=\"sum_distance\", initializer=\"zeros\")\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        y_pred_shape = tf.shape(y_pred)\n",
        "        batch_size = y_pred_shape[0]\n",
        "        logit_length = tf.fill([batch_size], y_pred_shape[1])\n",
        "        decoded, _ = tf.nn.ctc_greedy_decoder(\n",
        "            inputs=tf.transpose(y_pred, perm=[1, 0, 2]),\n",
        "            sequence_length=logit_length,\n",
        "        )\n",
        "        sum_distance = tf.math.reduce_sum(tf.edit_distance(decoded[0], y_true))\n",
        "        batch_size = tf.cast(batch_size, tf.float32)\n",
        "        self.sum_distance.assign_add(sum_distance)\n",
        "        self.total.assign_add(batch_size)\n",
        "\n",
        "    def result(self):\n",
        "        return self.sum_distance / self.total\n",
        "\n",
        "    def reset_states(self):\n",
        "        self.sum_distance.assign(0)\n",
        "        self.total.assign(0)\n"
      ],
      "metadata": {
        "id": "1PMRGxlqwtzY"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class CTCLoss(tf.keras.losses.Loss):\n",
        "    \"\"\"A class that wraps the function of tf.nn.ctc_loss.\n",
        "\n",
        "    Attributes:\n",
        "        logits_time_major: If False (default) , shape is [batch, time, logits],\n",
        "            If True, logits is shaped [time, batch, logits].\n",
        "        blank_index: Set the class index to use for the blank label. default is\n",
        "            -1 (num_classes - 1).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, logits_time_major=False, blank_index=-1, name=\"ctc_loss\"):\n",
        "        super().__init__(name=name)\n",
        "        self.logits_time_major = logits_time_major\n",
        "        self.blank_index = blank_index\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        \"\"\"Computes CTC (Connectionist Temporal Classification) loss. work on\n",
        "        CPU, because y_true is a SparseTensor.\n",
        "        \"\"\"\n",
        "        y_true = tf.cast(y_true, tf.int32)\n",
        "        y_pred_shape = tf.shape(y_pred)\n",
        "        logit_length = tf.fill([y_pred_shape[0]], y_pred_shape[1])\n",
        "        loss = tf.nn.ctc_loss(\n",
        "            labels=y_true,\n",
        "            logits=y_pred,\n",
        "            label_length=None,\n",
        "            logit_length=logit_length,\n",
        "            logits_time_major=self.logits_time_major,\n",
        "            blank_index=self.blank_index,\n",
        "        )\n",
        "        return tf.math.reduce_mean(loss)\n",
        "\n",
        "\n",
        "class CTCDecoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, table_path, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.table = tf.lookup.StaticHashTable(\n",
        "            tf.lookup.TextFileInitializer(\n",
        "                table_path,\n",
        "                tf.int64,\n",
        "                tf.lookup.TextFileIndex.LINE_NUMBER,\n",
        "                tf.string,\n",
        "                tf.lookup.TextFileIndex.WHOLE_LINE,\n",
        "            ),\n",
        "            \"\",\n",
        "        )\n",
        "\n",
        "    def detokenize(self, x):\n",
        "        x = tf.RaggedTensor.from_sparse(x)\n",
        "        x = tf.ragged.map_flat_values(self.table.lookup, x)\n",
        "        strings = tf.strings.reduce_join(x, axis=1)\n",
        "        return strings\n",
        "\n",
        "\n",
        "class CTCGreedyDecoder(CTCDecoder):\n",
        "    def __init__(self, table_path, merge_repeated=True, **kwargs):\n",
        "        super().__init__(table_path, **kwargs)\n",
        "        self.merge_repeated = merge_repeated\n",
        "\n",
        "    def call(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        sequence_length = tf.fill([input_shape[0]], input_shape[1])\n",
        "        decoded, neg_sum_logits = tf.nn.ctc_greedy_decoder(\n",
        "            tf.transpose(inputs, perm=[1, 0, 2]),\n",
        "            sequence_length,\n",
        "            self.merge_repeated,\n",
        "        )\n",
        "        strings = self.detokenize(decoded[0])\n",
        "        labels = tf.cast(decoded[0], tf.int32)\n",
        "        loss = tf.nn.ctc_loss(\n",
        "            labels=labels,\n",
        "            logits=inputs,\n",
        "            label_length=None,\n",
        "            logit_length=sequence_length,\n",
        "            logits_time_major=False,\n",
        "            blank_index=-1,\n",
        "        )\n",
        "        probability = tf.math.exp(-loss)\n",
        "        return strings, probability\n",
        "\n",
        "\n",
        "class CTCBeamSearchDecoder(CTCDecoder):\n",
        "    def __init__(self, table_path, beam_width=100, top_paths=1, **kwargs):\n",
        "        super().__init__(table_path, **kwargs)\n",
        "        self.beam_width = beam_width\n",
        "        self.top_paths = top_paths\n",
        "\n",
        "    def call(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        decoded, log_probability = tf.nn.ctc_beam_search_decoder(\n",
        "            tf.transpose(inputs, perm=[1, 0, 2]),\n",
        "            tf.fill([input_shape[0]], input_shape[1]),\n",
        "            self.beam_width,\n",
        "            self.top_paths,\n",
        "        )\n",
        "        strings = []\n",
        "        for i in range(self.top_paths):\n",
        "            strings.append(self.detokenize(decoded[i]))\n",
        "        strings = tf.concat(strings, 1)\n",
        "        probability = tf.math.exp(log_probability)\n",
        "        return strings, probability\n",
        "\n"
      ],
      "metadata": {
        "id": "JkblDXVhwt2k"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Configuration\n",
        "config = {\n",
        "    \"train\": {\n",
        "        \"dataset_builder\": {\n",
        "            \"table_path\": \"labels.txt\",\n",
        "            \"img_shape\": [32, 300, 3],  # Updated input size\n",
        "            \"max_img_width\": 300,\n",
        "            \"ignore_case\": True,\n",
        "        },\n",
        "        \"train_ann_paths\": [\"train_annotation.txt\"],\n",
        "        \"val_ann_paths\": [\"val_annotation.txt\"],\n",
        "        \"batch_size_per_replica\": 512,\n",
        "        \"epochs\": 20,\n",
        "        \"lr_schedule\": {\n",
        "            \"initial_learning_rate\": 0.0001,\n",
        "            \"decay_steps\": 600000,\n",
        "            \"alpha\": 0.01,\n",
        "        },\n",
        "        \"tensorboard\": {\n",
        "            \"histogram_freq\": 1,\n",
        "            \"profile_batch\": 0,\n",
        "        },\n",
        "    },\n",
        "}\n",
        "\n",
        "# Load configuration\n",
        "train_config = config[\"train\"]\n",
        "dataset_builder = DatasetBuilder(**train_config[\"dataset_builder\"])\n",
        "\n",
        "# Create datasets\n",
        "batch_size = train_config[\"batch_size_per_replica\"]\n",
        "train_ds = dataset_builder(train_config[\"train_ann_paths\"], batch_size, True)\n",
        "val_ds = dataset_builder(train_config[\"val_ann_paths\"], batch_size, False)\n",
        "\n",
        "# Distributed training strategy\n",
        "strategy = tf.distribute.MirroredStrategy()\n",
        "with strategy.scope():\n",
        "    # Build and compile the model\n",
        "    model = build_model(\n",
        "        dataset_builder.num_classes,\n",
        "        img_shape=train_config[\"dataset_builder\"][\"img_shape\"],\n",
        "    )\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(\n",
        "            tf.keras.optimizers.schedules.CosineDecay(**train_config[\"lr_schedule\"])\n",
        "        ),\n",
        "        loss=CTCLoss(),\n",
        "        metrics=[SequenceAccuracy()],\n",
        "    )\n",
        "\n",
        "# Model summary\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5EOqAaYwt5M",
        "outputId": "11f59f39-0b1c-4939-ce77-5ca8b13d914b"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"crnn\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_5 (InputLayer)        [(None, 32, 300, 3)]      0         \n",
            "                                                                 \n",
            " conv1 (Conv2D)              (None, 32, 300, 64)       1792      \n",
            "                                                                 \n",
            " pool1 (MaxPooling2D)        (None, 16, 150, 64)       0         \n",
            "                                                                 \n",
            " conv2 (Conv2D)              (None, 16, 150, 128)      73856     \n",
            "                                                                 \n",
            " pool2 (MaxPooling2D)        (None, 8, 75, 128)        0         \n",
            "                                                                 \n",
            " conv3 (Conv2D)              (None, 8, 75, 256)        294912    \n",
            "                                                                 \n",
            " bn3 (BatchNormalization)    (None, 8, 75, 256)        1024      \n",
            "                                                                 \n",
            " relu3 (Activation)          (None, 8, 75, 256)        0         \n",
            "                                                                 \n",
            " conv4 (Conv2D)              (None, 8, 75, 256)        590080    \n",
            "                                                                 \n",
            " pool4 (MaxPooling2D)        (None, 4, 75, 256)        0         \n",
            "                                                                 \n",
            " conv5 (Conv2D)              (None, 4, 75, 512)        1179648   \n",
            "                                                                 \n",
            " bn5 (BatchNormalization)    (None, 4, 75, 512)        2048      \n",
            "                                                                 \n",
            " relu5 (Activation)          (None, 4, 75, 512)        0         \n",
            "                                                                 \n",
            " conv6 (Conv2D)              (None, 4, 75, 512)        2359808   \n",
            "                                                                 \n",
            " pool6 (MaxPooling2D)        (None, 2, 75, 512)        0         \n",
            "                                                                 \n",
            " conv7 (Conv2D)              (None, 1, 74, 512)        1048576   \n",
            "                                                                 \n",
            " bn7 (BatchNormalization)    (None, 1, 74, 512)        2048      \n",
            "                                                                 \n",
            " relu7 (Activation)          (None, 1, 74, 512)        0         \n",
            "                                                                 \n",
            " reshape7 (Reshape)          (None, 74, 512)           0         \n",
            "                                                                 \n",
            " bi_lstm1 (Bidirectional)    (None, 74, 512)           1574912   \n",
            "                                                                 \n",
            " bi_lstm2 (Bidirectional)    (None, 74, 512)           1574912   \n",
            "                                                                 \n",
            " logits (Dense)              (None, 74, 94)            48222     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 8751838 (33.39 MB)\n",
            "Trainable params: 8749278 (33.38 MB)\n",
            "Non-trainable params: 2560 (10.00 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Model training\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.ModelCheckpoint(\n",
        "        \"checkpoints/{epoch}.h5\", save_weights_only=True\n",
        "    ),\n",
        "    tf.keras.callbacks.TensorBoard(log_dir=\"logs\", **train_config[\"tensorboard\"]),\n",
        "]\n",
        "\n",
        "model.fit(\n",
        "    train_ds,\n",
        "    epochs=train_config[\"epochs\"],\n",
        "    callbacks=callbacks,\n",
        "    validation_data=val_ds,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMDeZgOewt8D",
        "outputId": "1500bde0-0512-473b-d6f2-0f458a2e2358"
      },
      "execution_count": 51,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "     62/Unknown - 136s 1s/step - loss: 46.5583 - sequence_accuracy: 0.0000e+00"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:2723: UserWarning: Metric SequenceAccuracy implements a `reset_states()` method; rename it to `reset_state()` (without the final \"s\"). The name `reset_states()` has been deprecated to improve API consistency.\n",
            "  m.reset_state()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "62/62 [==============================] - 157s 2s/step - loss: 46.5583 - sequence_accuracy: 0.0000e+00 - val_loss: 29.7047 - val_sequence_accuracy: 0.0000e+00\n",
            "Epoch 2/20\n",
            "62/62 [==============================] - 83s 1s/step - loss: 28.0543 - sequence_accuracy: 0.0000e+00 - val_loss: 28.9362 - val_sequence_accuracy: 0.0000e+00\n",
            "Epoch 3/20\n",
            "62/62 [==============================] - 83s 1s/step - loss: 27.3750 - sequence_accuracy: 0.0000e+00 - val_loss: 28.6917 - val_sequence_accuracy: 0.0000e+00\n",
            "Epoch 4/20\n",
            "62/62 [==============================] - 87s 1s/step - loss: 27.1348 - sequence_accuracy: 0.0000e+00 - val_loss: 28.6692 - val_sequence_accuracy: 0.0000e+00\n",
            "Epoch 5/20\n",
            "62/62 [==============================] - 83s 1s/step - loss: 27.0296 - sequence_accuracy: 0.0000e+00 - val_loss: 28.4288 - val_sequence_accuracy: 0.0000e+00\n",
            "Epoch 6/20\n",
            "62/62 [==============================] - 84s 1s/step - loss: 26.9534 - sequence_accuracy: 0.0000e+00 - val_loss: 28.3629 - val_sequence_accuracy: 0.0000e+00\n",
            "Epoch 7/20\n",
            "62/62 [==============================] - 83s 1s/step - loss: 26.8644 - sequence_accuracy: 0.0000e+00 - val_loss: 28.5903 - val_sequence_accuracy: 0.0000e+00\n",
            "Epoch 8/20\n",
            "62/62 [==============================] - 83s 1s/step - loss: 26.6451 - sequence_accuracy: 0.0000e+00 - val_loss: 30.1395 - val_sequence_accuracy: 0.0000e+00\n",
            "Epoch 9/20\n",
            "62/62 [==============================] - 86s 1s/step - loss: 25.8867 - sequence_accuracy: 0.0000e+00 - val_loss: 35.5147 - val_sequence_accuracy: 0.0000e+00\n",
            "Epoch 10/20\n",
            "62/62 [==============================] - 83s 1s/step - loss: 23.5359 - sequence_accuracy: 0.0000e+00 - val_loss: 43.9401 - val_sequence_accuracy: 0.0000e+00\n",
            "Epoch 11/20\n",
            "62/62 [==============================] - 82s 1s/step - loss: 18.2769 - sequence_accuracy: 0.0000e+00 - val_loss: 53.8495 - val_sequence_accuracy: 0.0000e+00\n",
            "Epoch 12/20\n",
            "62/62 [==============================] - 83s 1s/step - loss: 11.9852 - sequence_accuracy: 0.0119 - val_loss: 59.6186 - val_sequence_accuracy: 0.0000e+00\n",
            "Epoch 13/20\n",
            "62/62 [==============================] - 83s 1s/step - loss: 7.0011 - sequence_accuracy: 0.1414 - val_loss: 60.2950 - val_sequence_accuracy: 0.0000e+00\n",
            "Epoch 14/20\n",
            "62/62 [==============================] - 83s 1s/step - loss: 4.1609 - sequence_accuracy: 0.3983 - val_loss: 51.2788 - val_sequence_accuracy: 0.0000e+00\n",
            "Epoch 15/20\n",
            "62/62 [==============================] - 83s 1s/step - loss: 2.6221 - sequence_accuracy: 0.6010 - val_loss: 43.2745 - val_sequence_accuracy: 0.0000e+00\n",
            "Epoch 16/20\n",
            "62/62 [==============================] - 83s 1s/step - loss: 2.0815 - sequence_accuracy: 0.6932 - val_loss: 40.2312 - val_sequence_accuracy: 6.2500e-04\n",
            "Epoch 17/20\n",
            "62/62 [==============================] - 83s 1s/step - loss: 1.5391 - sequence_accuracy: 0.7597 - val_loss: 13.6122 - val_sequence_accuracy: 0.0596\n",
            "Epoch 18/20\n",
            "62/62 [==============================] - 83s 1s/step - loss: 1.0539 - sequence_accuracy: 0.8328 - val_loss: 3.4032 - val_sequence_accuracy: 0.2359\n",
            "Epoch 19/20\n",
            "62/62 [==============================] - 86s 1s/step - loss: 0.8252 - sequence_accuracy: 0.8716 - val_loss: 1.9318 - val_sequence_accuracy: 0.5199\n",
            "Epoch 20/20\n",
            "62/62 [==============================] - 83s 1s/step - loss: 0.6760 - sequence_accuracy: 0.8946 - val_loss: 1.2146 - val_sequence_accuracy: 0.7089\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7a866b707370>"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Continue training from a saved checkpoint\n",
        "saved_weights_path = \"checkpoints/20.h5\"  # Adjust the path accordingly\n",
        "model.load_weights(saved_weights_path)\n",
        "\n",
        "# Model training\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.ModelCheckpoint(\n",
        "        \"checkpoints/{epoch}.h5\", save_weights_only=True\n",
        "    ),\n",
        "    tf.keras.callbacks.TensorBoard(log_dir=\"logs\", **train_config[\"tensorboard\"]),\n",
        "]\n",
        "# Continue training on new data\n",
        "model.fit(\n",
        "    train_ds,\n",
        "    epochs=train_config[\"epochs\"],\n",
        "    callbacks=callbacks,\n",
        "    validation_data=val_ds,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AaS-8YDEwt-j",
        "outputId": "4c7661e7-f172-40d2-d342-0e5f231f43d3"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:2723: UserWarning: Metric SequenceAccuracy implements a `reset_states()` method; rename it to `reset_state()` (without the final \"s\"). The name `reset_states()` has been deprecated to improve API consistency.\n",
            "  m.reset_state()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "62/62 [==============================] - 90s 1s/step - loss: 0.5583 - sequence_accuracy: 0.9183 - val_loss: 0.8785 - val_sequence_accuracy: 0.8199\n",
            "Epoch 2/20\n",
            "62/62 [==============================] - 83s 1s/step - loss: 0.4653 - sequence_accuracy: 0.9346 - val_loss: 0.7833 - val_sequence_accuracy: 0.8303\n",
            "Epoch 3/20\n",
            "62/62 [==============================] - 83s 1s/step - loss: 0.3928 - sequence_accuracy: 0.9487 - val_loss: 0.8091 - val_sequence_accuracy: 0.8091\n",
            "Epoch 4/20\n",
            "62/62 [==============================] - 83s 1s/step - loss: 0.4407 - sequence_accuracy: 0.9402 - val_loss: 3.3539 - val_sequence_accuracy: 0.4194\n",
            "Epoch 5/20\n",
            "62/62 [==============================] - 83s 1s/step - loss: 0.3317 - sequence_accuracy: 0.9558 - val_loss: 0.7609 - val_sequence_accuracy: 0.8257\n",
            "Epoch 6/20\n",
            "62/62 [==============================] - 83s 1s/step - loss: 0.4826 - sequence_accuracy: 0.9352 - val_loss: 11.6046 - val_sequence_accuracy: 0.0882\n",
            "Epoch 7/20\n",
            "62/62 [==============================] - 83s 1s/step - loss: 0.3280 - sequence_accuracy: 0.9531 - val_loss: 0.5997 - val_sequence_accuracy: 0.8719\n",
            "Epoch 8/20\n",
            "62/62 [==============================] - 87s 1s/step - loss: 0.2125 - sequence_accuracy: 0.9756 - val_loss: 0.5844 - val_sequence_accuracy: 0.8746\n",
            "Epoch 9/20\n",
            "62/62 [==============================] - 83s 1s/step - loss: 0.1887 - sequence_accuracy: 0.9809 - val_loss: 3.0121 - val_sequence_accuracy: 0.6890\n",
            "Epoch 10/20\n",
            "62/62 [==============================] - 84s 1s/step - loss: 0.2724 - sequence_accuracy: 0.9629 - val_loss: 0.5444 - val_sequence_accuracy: 0.8654\n",
            "Epoch 11/20\n",
            "62/62 [==============================] - 83s 1s/step - loss: 0.1475 - sequence_accuracy: 0.9843 - val_loss: 0.4969 - val_sequence_accuracy: 0.8913\n",
            "Epoch 12/20\n",
            "62/62 [==============================] - 83s 1s/step - loss: 0.1179 - sequence_accuracy: 0.9894 - val_loss: 0.4943 - val_sequence_accuracy: 0.8855\n",
            "Epoch 13/20\n",
            "62/62 [==============================] - 83s 1s/step - loss: 0.1013 - sequence_accuracy: 0.9919 - val_loss: 0.4267 - val_sequence_accuracy: 0.8991\n",
            "Epoch 14/20\n",
            "62/62 [==============================] - 83s 1s/step - loss: 0.0874 - sequence_accuracy: 0.9938 - val_loss: 0.4261 - val_sequence_accuracy: 0.9032\n",
            "Epoch 15/20\n",
            "62/62 [==============================] - 83s 1s/step - loss: 0.0786 - sequence_accuracy: 0.9946 - val_loss: 0.4389 - val_sequence_accuracy: 0.8986\n",
            "Epoch 16/20\n",
            "62/62 [==============================] - 84s 1s/step - loss: 0.0700 - sequence_accuracy: 0.9959 - val_loss: 0.4429 - val_sequence_accuracy: 0.8986\n",
            "Epoch 17/20\n",
            "62/62 [==============================] - 83s 1s/step - loss: 0.0635 - sequence_accuracy: 0.9963 - val_loss: 0.4095 - val_sequence_accuracy: 0.9043\n",
            "Epoch 18/20\n",
            "62/62 [==============================] - 84s 1s/step - loss: 0.0576 - sequence_accuracy: 0.9970 - val_loss: 0.3891 - val_sequence_accuracy: 0.9093\n",
            "Epoch 19/20\n",
            "62/62 [==============================] - 83s 1s/step - loss: 0.0531 - sequence_accuracy: 0.9970 - val_loss: 0.3831 - val_sequence_accuracy: 0.9129\n",
            "Epoch 20/20\n",
            "62/62 [==============================] - 83s 1s/step - loss: 0.0471 - sequence_accuracy: 0.9980 - val_loss: 0.3911 - val_sequence_accuracy: 0.9106\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7a8668152a10>"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OZSHEcl5wuA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MeZdxS1XwuD3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}